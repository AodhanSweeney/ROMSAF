{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import CubicSpline\n",
    "from datetime import datetime\n",
    "from datetime import date, timedelta\n",
    "import cdsapi\n",
    "import sys\n",
    "\n",
    "monthString = 'december'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_to_local(hour, lon):\n",
    "    viable_times = np.linspace(0,23,24)\n",
    "    lt_change = (np.deg2rad(lon)/np.pi)*12\n",
    "    lt = hour + np.round(lt_change)\n",
    "    lts = lt.astype(int)\n",
    "    lts = lts%24\n",
    "    return lts\n",
    "\n",
    "def TLS(temp, weight, levels):\n",
    "    TpWp = temp*weight\n",
    "    \n",
    "    integral_top = np.trapz(TpWp, levels)\n",
    "    integral_bottom = np.trapz(weight, levels)\n",
    "    \n",
    "    return(integral_top/integral_bottom)\n",
    "\n",
    "def area_weight_latitude_bin(era_5_raw_temp_profiles, latitude_bins):\n",
    "    weights = np.cos(np.deg2rad(latitude_bins))\n",
    "    weighted_era_5_temp_profiles = era_5_raw_temp_profiles*weights[np.newaxis,:,np.newaxis]\n",
    "    \n",
    "    reshaped_weighted_era_5_temp_profiles = np.reshape(weighted_era_5_temp_profiles, (19,20*1440))\n",
    "    sum_weighted_era_5_temp_profiles = np.sum(reshaped_weighted_era_5_temp_profiles, axis=1)\n",
    "    sum_of_all_weights = np.sum(weights)*1440\n",
    "    \n",
    "    area_weighted_average_era_5_temp_profile = sum_weighted_era_5_temp_profiles/sum_of_all_weights\n",
    "    \n",
    "    \n",
    "    return(area_weighted_average_era_5_temp_profile)\n",
    "\n",
    "def occultation_location(path_item):\n",
    "    ds = Dataset(path_item)\n",
    "    \n",
    "    if ds.bad == '0':\n",
    "        alts = ds.variables['MSL_alt'][:]\n",
    "        lat = float(ds.variables['Lat'][174]) #lat and lon are choosen as index 200 because-\n",
    "        lon = float(ds.variables['Lon'][174]) #MSU T4 weighting function peaks at 20km\n",
    "        hour = ds.hour\n",
    "        day_of_month = ds.day\n",
    "        occultation = [lat, lon, hour, day_of_month]\n",
    "        ds.close()\n",
    "        return occultation\n",
    "    \n",
    "def find_idx_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def colocations(path_to_ERA5_data, year_string_idx):\n",
    "    path_to_monthly_ERA5_mean = Path(path_to_ERA5_data)\n",
    "    month_of_data_ds = Dataset(path_to_monthly_ERA5_mean)\n",
    "\n",
    "    hours_since_1900 = int(month_of_data_ds['time'][0]) \n",
    "    days_since_1900 = int(hours_since_1900/24)\n",
    "    start = date(1900,1,1) #ERA-5 data counts from 1900  \n",
    "    delta = timedelta(days_since_1900)  # delta is the time since 1900\n",
    "    offset = str(start + delta)  # combine the two to get actual time you want\n",
    "    date_thing = datetime.strptime(offset, '%Y-%m-%d')\n",
    "    day_of_year = date.timetuple(date_thing).tm_yday\n",
    "    year = offset[:4]\n",
    "    print(year, day_of_year)\n",
    "    print(offset)\n",
    "\n",
    "    latitudes = month_of_data_ds['latitude'][:]\n",
    "    longitudes = month_of_data_ds['longitude'][:]\n",
    "    times = month_of_data_ds['time'][:]\n",
    "\n",
    "    days_in_month = int(len(times)/24)\n",
    "\n",
    "    utc_times = times%24\n",
    "\n",
    "    p_level = month_of_data_ds['level'][:]\n",
    "    p_level_positive_z = np.flip(p_level)\n",
    "\n",
    "    #Create a local time hour array for each UTC hour\n",
    "    lt_hour_nd_array = []\n",
    "    for hour in range(0,24):\n",
    "        local_hours_from_lon = utc_to_local(hour, longitudes[:])\n",
    "        new_lt_array = np.broadcast_to(local_hours_from_lon, (19, 721, 1440))\n",
    "        lt_hour_nd_array.append(new_lt_array)\n",
    "    lt_hour_nd_array = np.array(lt_hour_nd_array)\n",
    "    hour_array = np.arange(0,24)\n",
    "\n",
    "\n",
    "    #Import the MSU T4 weighting function and its respective pressure levels\n",
    "    pressure_levels_msuT4 = np.load('MSU_T4_weighting_pressure_levels.npy')\n",
    "    weighting_func_msuT4 = np.load('MSU_T4_weighting_function.npy')\n",
    "\n",
    "    #only take range from 450hPa --> 3hPa matching the COSMIC used altitude levels\n",
    "    presure_levels_correct_range = pressure_levels_msuT4[(2.9<= pressure_levels_msuT4) & (pressure_levels_msuT4< 360)]\n",
    "    weighting_func_correct_range = weighting_func_msuT4[(2.9<= pressure_levels_msuT4) & (pressure_levels_msuT4< 360)]\n",
    "    pressure_levels_correct_range_logrithmic = np.geomspace(presure_levels_correct_range[-1], presure_levels_correct_range[0], num=320)\n",
    "\n",
    "    #weighting function needs to be interpolated to this new set of pressure levels\n",
    "    weighting_func_interpolater = CubicSpline(np.flip(presure_levels_correct_range), np.flip(weighting_func_correct_range))\n",
    "    pressure_levels_correct_range_logrithmic_positive_z = np.flip(pressure_levels_correct_range_logrithmic)\n",
    "    weighting_func_logrithmic_positive_z = weighting_func_interpolater(pressure_levels_correct_range_logrithmic_positive_z)\n",
    "\n",
    "\n",
    "    #create the pressure spacing for weighting function integration\n",
    "    dp_increments = [presure_levels_correct_range[p] - presure_levels_correct_range[p-1] for p in range(1, len(presure_levels_correct_range))]\n",
    "\n",
    "    #Now we need to load data from all the various satellites\n",
    "    cosmic = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_cosmic_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    metopa = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_metop_A_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    metopb = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_metop_B_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    grace = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_grace_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    tsx = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_tsx_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    kompsat5 = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_kompsat5_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    paz = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_paz_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    cosmic2 = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_cosmic2_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    sacc = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_sacc_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    tdx = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_tdx_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "    metopc = np.load('/usb/monthly_diurnal_cycle_data_occultations/{monthString}_metopc_diurnal_cycles_TLS_year_info.npy'.format(monthString=monthString), allow_pickle=True)\n",
    "\n",
    "    gpsro_data = np.concatenate((cosmic.T, metopa.T, metopb.T, grace.T, tsx.T, kompsat5.T, paz.T, cosmic2.T, sacc.T, tdx.T, metopc.T))\n",
    "    gpsro_data_df = pd.DataFrame(gpsro_data, columns=['Lat', 'Lon', 'Year', 'Day', 'Hour', 'Temp'])\n",
    "    gpsro_data_df.Hour  = gpsro_data_df['Hour'].apply(np.round).astype(int)\n",
    "    year_gpsro_df = gpsro_data_df[gpsro_data_df['Year'] == int(year)]\n",
    "\n",
    "    ERA_5_monthly_mean_synthetic_TLS_map = []\n",
    "    for t in range(0, days_in_month):\n",
    "        day_start = t*24\n",
    "        day_end = day_start+24\n",
    "\n",
    "        day_of_month = int(day_start/24) + day_of_year\n",
    "        print('Day: ', day_of_month)\n",
    "\n",
    "        day_of_data = month_of_data_ds['t'][day_start:day_end,:,:,:]\n",
    "\n",
    "        day_of_gpsro = year_gpsro_df[year_gpsro_df['Day'] == day_of_month]\n",
    "\n",
    "\n",
    "        for h_idx in range(0, 24):\n",
    "            hour = hour_array[h_idx]\n",
    "            print('Hour: ', hour)\n",
    "            #for whichever local time hour you want, you need to change ERA-5 from UTC to LT\n",
    "            boolean_lt_hour_map = lt_hour_nd_array == int(hour) #this creates a boolean grid for correct lt\n",
    "            boolean_lt_hour_mask = np.abs(boolean_lt_hour_map - 1) #this is a mask for all wrong local times\n",
    "            day_of_data_masked = np.ma.masked_array(day_of_data, boolean_lt_hour_mask, fill_value=np.NaN) #apply mask\n",
    "            day_of_data_w_nans = day_of_data_masked.filled() # return an array with data from LT hour\n",
    "            lt_hour_data_array = np.nansum(day_of_data_w_nans, axis=0) #all masked values are nan, sum along utc axis\n",
    "\n",
    "            hour_of_occultations = day_of_gpsro[day_of_gpsro['Hour'] == hour]\n",
    "            hour_of_occultations = hour_of_occultations.reset_index(drop=True)\n",
    "\n",
    "            for occultation in hour_of_occultations.iterrows():\n",
    "                occultation_instance = occultation[1]\n",
    "                \n",
    "                occult_lat = occultation_instance.Lat\n",
    "                occult_lon = occultation_instance.Lon\n",
    "\n",
    "                closest_lat_idx = find_idx_nearest(latitudes, occult_lat)\n",
    "                if occult_lon >= 0:\n",
    "                    closest_lon_idx = find_idx_nearest(longitudes, occult_lon)\n",
    "                elif occult_lon < 0:\n",
    "                    closest_lon_idx = find_idx_nearest(longitudes, 360 + occult_lon)\n",
    "\n",
    "                #get closest profile in ERA-5 that corresponds to the occultation\n",
    "                era_5_colocated_temp_prof = lt_hour_data_array[:,closest_lat_idx,closest_lon_idx] - 273.15\n",
    "\n",
    "                #interpolate using a cubic spline\n",
    "                temperature_interpolater = CubicSpline(p_level, era_5_colocated_temp_prof)\n",
    "                era5_temp_prof_interp = temperature_interpolater(pressure_levels_correct_range_logrithmic)\n",
    "\n",
    "                \n",
    "                #flip the interpolated profile from 2.9hPa-->351hPa to 351hPa-->2.9hPa\n",
    "                temp_prof_interp_positive_z = np.flip(era5_temp_prof_interp)\n",
    "\n",
    "                #Find TLS temp\n",
    "                ERA5_TLS_temp = TLS(temp_prof_interp_positive_z, \n",
    "                                             weighting_func_logrithmic_positive_z, \n",
    "                                             pressure_levels_correct_range_logrithmic_positive_z)\n",
    "\n",
    "                ERA5_LT_TLS_temp = [day_of_month, hour, year, latitudes[closest_lat_idx], \n",
    "                                    longitudes[closest_lon_idx], ERA5_TLS_temp]\n",
    "                ERA_5_monthly_mean_synthetic_TLS_map.append(ERA5_LT_TLS_temp)\n",
    "\n",
    "    monthly_synthetic_TLS_map = pd.DataFrame(ERA_5_monthly_mean_synthetic_TLS_map, columns=['Day', 'Hour', 'Year', 'Lat', 'Lon', 'Temp'])\n",
    "    np.save('colocations_fixed_longitude/{monthString}_{year_idx}_ERA_5_colocated_occultations'.format(monthString=monthString,\n",
    "        year_idx=year_string_idx), monthly_synthetic_TLS_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_as_strings = ['january', 'february', 'march', 'april', 'may', 'june', 'july', \n",
    "                     'august', 'september', 'october', 'november', 'december']\n",
    "for year in range(2001, 2023):\n",
    "    for month in range(1, 13):\n",
    "        year_string_idx = str(year)\n",
    "        path_to_colocations = '/home/disk/pna2/aodhan/ERA5_hourly_data/{monthString}_{year_idx}_ERA5.nc'.format(\n",
    "            monthString=monthString, year_idx=year_string_idx)\n",
    "        colocations(path_to_colocations, year_string_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/bdc2/aodhan/ROM_SAF/www.romsaf.org/pub/cdr/v1.0/profs/*/atm/'\n",
    "cdr_TLS_maps = []\n",
    "for year in range(2001, 2017):\n",
    "    year_of_TLS_path = base_path + str(year) + '/*TLS*'\n",
    "    TLS_yearly_files = glob.glob(year_of_TLS_path)\n",
    "    one_year_TLS_files = []\n",
    "    for sat_file in TLS_yearly_files:\n",
    "        one_sat_one_year_TLS = np.load(sat_file, allow_pickle=True)\n",
    "        one_year_TLS_files.append(one_sat_one_year_TLS)\n",
    "    one_year_TLS_files = np.concatenate(one_year_TLS_files, axis=0)\n",
    "    one_year_df = pd.DataFrame(one_year_TLS_files, columns=['Month', 'Lat', 'Lon', 'Date', 'TLS'])\n",
    "    print(one_year_df)\n",
    "    break\n",
    "    one_year_df['latbin'] = one_year_df.Lat.apply(to_bin_lat)\n",
    "    one_year_df['lonbin'] = one_year_df.Lon.apply(to_bin_lon)\n",
    "    year_of_TLS_maps = []\n",
    "    for month in range(1, 13):\n",
    "        one_month_df = one_year_df[one_year_df['Month']==month]\n",
    "        mean_TLS_map = []\n",
    "        for lat_idx in lats:\n",
    "            one_month_df_at_lat = one_month_df[one_month_df['latbin'] == lat_idx]\n",
    "            mean_TLS_at_lat = []\n",
    "            for lon_idx in lons:\n",
    "                one_month_df_box = one_month_df_at_lat[one_month_df_at_lat['lonbin'] == lon_idx]\n",
    "                if one_month_df_box.size > 0:\n",
    "                    mean_TLS = one_month_df_box.TLS.mean()\n",
    "                elif one_month_df_box.size == 0:\n",
    "                    mean_TLS = np.NaN\n",
    "                else:\n",
    "                    print('Size of df is invalid.')\n",
    "                mean_TLS_at_lat.append(mean_TLS)\n",
    "            mean_TLS_map.append(mean_TLS_at_lat)\n",
    "        year_of_TLS_maps.append(mean_TLS_map)\n",
    "    cdr_TLS_maps.append(year_of_TLS_maps)\n",
    "\n",
    "\n",
    "#np.load('/home/bdc2/aodhan/ROM_SAF/www.romsaf.org/pub/cdr/v1.0/profs/champ/atm/2003/champ_TLS_measurements.npy', allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
